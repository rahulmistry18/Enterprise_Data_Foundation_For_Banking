# In file: dags/core_banking_etl_dag.py

from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.operators.python import PythonOperator
from airflow.models.dag import DAG
from datetime import datetime, timedelta
import os
import sys

# Add the 'scripts' directory to the Python path for imports
# This is necessary because Airflow runs Python code from within the container environment
sys.path.append(os.path.join(os.environ['AIRFLOW_HOME'], 'scripts'))
from spark_jobs.transformer import SparkTransformer # Import the Spark class

# --- Configuration ---
POSTGRES_CONN_ID = "postgres_banking_dw"
# Use macros for run-specific data, especially for output paths
TODAY_DATE_STR = "{{ ds_nodash }}" 

# Helper functions that will be called by PythonOperator
def run_spark_transform_job():
    """
    Initializes a SparkSession, runs the PySpark transformation, 
    and writes the output to a temporary Parquet staging area (XCom).
    """
    from pyspark.sql import SparkSession
    
    # Initialize SparkSession 
    spark = SparkSession.builder.appName("CoreBankingSparkJob").getOrCreate()
    
    # Execute the PySpark logic
    transformer = SparkTransformer(spark)
    fact_tx_df = transformer.run_pipeline()
    
    # Write data back to a temporary staging location 
    temp_output_path = os.path.join(os.environ['AIRFLOW_HOME'], 'temp_spark_output', TODAY_DATE_STR)
    fact_tx_df.write.mode("overwrite").parquet(temp_output_path)
    
    spark.stop()
    # XCom Push: Return the output path for the next task to pull
    return temp_output_path 

def load_fact_data_to_dwh(ti):
    """
    Pulls the Spark output path (via XCom), reads the Parquet data, 
    and loads it into the PostgreSQL staging table (stg_transaction_data).
    """
    import pandas as pd
    from pyspark.sql import SparkSession
    from airflow.providers.postgres.hooks.postgres import PostgresHook

    spark = SparkSession.builder.appName("PostgresLoader").getOrCreate()
    pg_hook = PostgresHook(postgres_conn_id=POSTGRES_CONN_ID)
    
    # XCom Pull: Retrieve the path generated by the previous Spark task
    temp_output_path = ti.xcom_pull(task_ids='transform_and_quality_check')
    
    # Read the parquet data back into a Spark DataFrame
    fact_tx_df = spark.read.parquet(temp_output_path)
    
    # Convert Spark DataFrame to Pandas for loading into Postgres
    fact_tx_pd = fact_tx_df.toPandas()
    
    # Use Postgres Hook to insert the data
    pg_hook.insert_rows(
        table="stg_transaction_data",
        rows=fact_tx_pd[['transaction_id', 'account_id', 'transaction_date', 'transaction_amount', 'transaction_type', 'source_channel']].values.tolist(),
        target_fields=['transaction_id', 'account_id', 'transaction_date', 'amount', 'type', 'source_channel']
    )
    spark.stop()


with DAG(
    dag_id='core_banking_analytics_etl',
    start_date=datetime(2025, 1, 1),
    schedule=timedelta(days=1),  # Run daily
    catchup=False,
    tags=['core_banking', 'spark', 'governance'],
    default_args={
        'owner': 'airflow',
        'retries': 3,
        'retry_delay': timedelta(minutes=5),
    }
) as dag:
    
    # --- 1. Load Static Data (Dimensions) ---
    # Load customer master data (simple CSV to Staging Table)
    load_customer_data = PostgresOperator(
        task_id='load_stg_customer_data',
        postgres_conn_id=POSTGRES_CONN_ID,
        # NOTE: COPY is the fastest way to load CSV into Postgres
        sql=f"""
            COPY stg_customer_master
            FROM '/opt/airflow/source_data/customer_master.csv'
            DELIMITER ',' CSV HEADER;
        """
    )

    # Transform Staging Customer to Dimension Table
    transform_dim_customer = PostgresOperator(
        task_id='transform_dim_customer',
        postgres_conn_id=POSTGRES_CONN_ID,
        sql=f"""
            -- Insert new customers, update existing customers (basic SCD Type 1 behavior)
            INSERT INTO dim_customer (customer_id, gender, region, joining_date, is_premium, valid_from, is_current)
            SELECT
                customer_id, gender, region, joining_date, is_premium, NOW(), TRUE
            FROM stg_customer_master
            ON CONFLICT (customer_id) DO NOTHING; 

            -- Load static account data (simple insert/ignore for now)
            COPY dim_account (account_id, customer_id, account_type)
            FROM '/opt/airflow/source_data/account_balances.csv' 
            DELIMITER ',' CSV HEADER;
        """
    )
    
    # --- 2. PySpark Transformation & Data Quality Check ---
    transform_and_quality_check = PythonOperator(
        task_id='transform_and_quality_check',
        python_callable=run_spark_transform_job,
    )

    # --- 3. Load Fact Data to DWH ---
    load_stg_fact_data = PythonOperator(
        task_id='load_stg_fact_data',
        python_callable=load_fact_data_to_dwh,
    )
    
    # --- 4. Final Fact Table Insertion and Data Mart Creation ---
    insert_fact_table = PostgresOperator(
        task_id='insert_fact_table',
        postgres_conn_id=POSTGRES_CONN_ID,
        sql=f"""
            -- Insert into the partitioned fact table
            INSERT INTO fact_transactions (transaction_id, account_pk, transaction_date, transaction_amount, transaction_type, source_channel)
            SELECT
                st.transaction_id,
                da.account_pk,
                st.transaction_date,
                st.amount,
                st.type,
                st.source_channel
            FROM stg_transaction_data st
            JOIN dim_account da ON st.account_id = da.account_id
            ON CONFLICT (transaction_id) DO NOTHING; 

            -- Regulatory/Governance Audit Log: TRUNCATE staging table after successful load
            TRUNCATE TABLE stg_transaction_data;
        """
    )

    # Create/Refresh BI Data Mart View
    create_data_mart = PostgresOperator(
        task_id='create_bi_data_mart',
        postgres_conn_id=POSTGRES_CONN_ID,
        sql=os.path.join(SQL_PATH, 'dm_account_activity.sql'),
    )

    # --- Set Dependencies (Defining the flow) ---
    load_customer_data >> transform_dim_customer
    transform_dim_customer >> transform_and_quality_check 
    transform_and_quality_check >> load_stg_fact_data
    load_stg_fact_data >> insert_fact_table 
    insert_fact_table >> create_data_mart